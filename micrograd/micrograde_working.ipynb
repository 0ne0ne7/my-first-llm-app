{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "663dfdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f3179e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child,v))\n",
    "                build(child)\n",
    "\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir':'LR'})\n",
    "\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "\n",
    "        dot.node(name=uid, label=\"{% s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad),shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name=uid+n._op, label=n._op)\n",
    "            dot.edge(uid+n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)),str(id(n2))+n2._op)\n",
    "\n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3ae12c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad # Corrected\n",
    "            other.grad += self.data * out.grad # Corrected\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data**(other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad # Corrected\n",
    "        out._backward = _backward # Corrected\n",
    "        return out\n",
    "\n",
    "    def __neg__(self): \n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return other * self**-1\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5d5e37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self,nin):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self,x):\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w,x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs)==1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            ps = neuron.parameters()\n",
    "            params.extend(ps)\n",
    "        return params\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i],sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7a9b9ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.287531305789408)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0,3.0,-1.0]\n",
    "n = MLP(3, [4,3,1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d4b4dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0,3.0,-4.0],\n",
    "    [3.0, -1.0, 2.0],\n",
    "    [5.0,6.0,-3.0]\n",
    "]\n",
    "ys = [1.0, -1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cb09f842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 4.6603969167011305\n",
      "step 1 loss 1.1938075476560992\n",
      "step 2 loss 0.44837103742362483\n",
      "step 3 loss 0.23637641798456208\n",
      "step 4 loss 0.15463636766268396\n",
      "step 5 loss 0.11316504610712137\n",
      "step 6 loss 0.08849037172051513\n",
      "step 7 loss 0.07226554035956047\n",
      "step 8 loss 0.0608457724275088\n",
      "step 9 loss 0.05240290067988227\n",
      "step 10 loss 0.045924557199500005\n",
      "step 11 loss 0.04080702656181902\n",
      "step 12 loss 0.0366689726947795\n",
      "step 13 loss 0.03325819474769501\n",
      "step 14 loss 0.030401508979218426\n",
      "step 15 loss 0.02797620995697951\n",
      "step 16 loss 0.025893024175523027\n",
      "step 17 loss 0.024085510081040654\n",
      "step 18 loss 0.022503236252516448\n",
      "step 19 loss 0.02110725878155211\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "\n",
    "    # Forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum((yout-ygt)**2 for yout,ygt in zip(ypred,ys))\n",
    "\n",
    "    # Backward pass\n",
    "    for p in n.parameters():\n",
    "        p.grad=0.0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "    print(f\"step {k} loss {loss.data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5278df51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Prediction: [0. 0. 0. 0.]\n",
      "Actual Target: [2. 4. 6. 8.]\n",
      "Loss: 30.0\n",
      "--- Gradients ---\n",
      "Gradient of w: -30.0\n",
      "Gradient of b: -10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Create Tensors\n",
    "# x is our input data\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "\n",
    "# y is our target output data\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# w and b are the parameters of our model, which we want to learn.\n",
    "# We initialize them with random values and set requires_grad=True\n",
    "# to tell PyTorch we want to compute gradients for them.\n",
    "w = torch.tensor([[0.0]], requires_grad=True)\n",
    "b = torch.tensor([[0.0]], requires_grad=True)\n",
    "\n",
    "# 2. Define the Model and Loss Function\n",
    "# This is a simple linear model: y_pred = x * w + b\n",
    "def forward(x):\n",
    "    return x @ w + b # @ is matrix multiplication in PyTorch\n",
    "\n",
    "# We use Mean Squared Error (MSE) as our loss function\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "# 3. Perform a Forward Pass and Calculate Loss\n",
    "# Get the model's prediction\n",
    "y_pred = forward(x)\n",
    "\n",
    "# Calculate how wrong the prediction is\n",
    "L = loss(y, y_pred)\n",
    "\n",
    "# 4. Automatic Differentiation (The Magic Part!)\n",
    "# PyTorch will now automatically calculate the gradients of the loss\n",
    "# with respect to all tensors that have requires_grad=True (w and b).\n",
    "L.backward()\n",
    "\n",
    "# The calculated gradients are stored in the .grad attribute of the tensors.\n",
    "print(f\"Initial Prediction: {y_pred.detach().numpy().flatten()}\")\n",
    "print(f\"Actual Target: {y.numpy().flatten()}\")\n",
    "print(f\"Loss: {L.item()}\")\n",
    "print(\"--- Gradients ---\")\n",
    "print(f\"Gradient of w: {w.grad.item()}\")\n",
    "print(f\"Gradient of b: {b.grad.item()}\")\n",
    "\n",
    "# In a real training loop, you would now use these gradients\n",
    "# to update the weights w and b and repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625d1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai-venv)",
   "language": "python",
   "name": "genai-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
